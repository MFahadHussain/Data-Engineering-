{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8SSFpg0KoGgfzghm3GKwa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MFahadHussain/Data-Engineering-/blob/main/WebScrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python Script\n",
        "You'll need to install the necessary libraries if you haven't already:\n"
      ],
      "metadata": {
        "id": "gNDl3EPEmeP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDvSYjMvmgJ3",
        "outputId": "0039c7d9-9477-4ea1-d8b9-c73d4793f50b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Function to fetch the content of the webpage\n",
        "def fetch_quotes(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all quotes on the page\n",
        "    quotes = []\n",
        "    for quote in soup.find_all('div', class_='quote'):\n",
        "        text = quote.find('span', class_='text').get_text()\n",
        "        author = quote.find('small', class_='author').get_text()\n",
        "        tags = [tag.get_text() for tag in quote.find_all('a', class_='tag')]\n",
        "        quotes.append({'quote': text, 'author': author, 'tags': tags})\n",
        "\n",
        "    return quotes\n",
        "\n",
        "# Main function to save the quotes into a CSV\n",
        "def save_to_csv(quotes, filename='quotes.csv'):\n",
        "    # Convert the list of quotes to a DataFrame\n",
        "    df = pd.DataFrame(quotes)\n",
        "\n",
        "    # Check if CSV exists, if so, append, otherwise create new\n",
        "    try:\n",
        "        df_existing = pd.read_csv(filename)\n",
        "        df_existing = df_existing.append(df, ignore_index=True)\n",
        "        df_existing.to_csv(filename, index=False)\n",
        "    except FileNotFoundError:\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "# Main scraping flow\n",
        "def main():\n",
        "    base_url = 'http://quotes.toscrape.com/'\n",
        "    all_quotes = []\n",
        "\n",
        "    # Scrape 10 pages for demonstration\n",
        "    for page_num in range(1, 11):\n",
        "        url = base_url + str(page_num)\n",
        "        print(f\"Scraping page {page_num}...\")\n",
        "        quotes = fetch_quotes(url)\n",
        "        all_quotes.extend(quotes)\n",
        "        time.sleep(2)  # To avoid hitting the server too hard\n",
        "\n",
        "    # Save all the quotes to CSV\n",
        "    save_to_csv(all_quotes)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnhsUV3wmmgb",
        "outputId": "fa59328f-e8b3-4f65-d372-2338daf56a17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "fetch_quotes(url): Scrapes the quotes from the webpage and returns them as a list of dictionaries.\n",
        "\n",
        "save_to_csv(quotes, filename): Converts the list of dictionaries to a Pandas DataFrame and saves the data to a CSV file. If the file already exists, it appends new data.\n",
        "\n",
        "main(): This part controls the scraping loop, iterating over multiple pages of quotes."
      ],
      "metadata": {
        "id": "whTYo35jm8Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Function to convert JSON file to CSV\n",
        "def json_to_csv(input_json_file, output_csv_file):\n",
        "    # Load the JSON data\n",
        "    with open(input_json_file, 'r') as f:\n",
        "        data = json.load(f)  # Load the data from the JSON file\n",
        "\n",
        "    # Convert the JSON data into a pandas DataFrame\n",
        "    df = pd.json_normalize(data)  # Flatten nested JSON if needed\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"Data has been successfully saved to {output_csv_file}\")\n",
        "\n",
        "# Example usage\n",
        "input_json_file = 'data.json'  # Replace with your JSON file path\n",
        "output_csv_file = 'report.csv'  # Desired output CSV file name\n",
        "\n",
        "json_to_csv(input_json_file, output_csv_file)\n"
      ],
      "metadata": {
        "id": "RX0ijKcAnxT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Automating with Cron Job\n",
        "If you want this script to run periodically, you can set up a cron job in Linux.\n",
        "\n",
        "Hereâ€™s how you can add a cron job to run this script every day at 8 AM:\n",
        "\n",
        "Open the cron table:"
      ],
      "metadata": {
        "id": "xnDfYMdLnB_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#crontab -e\n",
        "\n",
        "#0 8 * * * /usr/bin/python3 /path/to/your/script.py\n",
        " # IN BASH"
      ],
      "metadata": {
        "id": "qBnn58vLmyEX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zXhuqbL9m7S1"
      }
    }
  ]
}